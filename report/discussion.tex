%!TEX root = main.tex
\section{Discussion}
\subsection{TIMIT \& ELSDR}
In the sex classification case (\cref{tab:results-sex}) it is seen that the network models perform better than the GMM on MFCC for both TIMIT and ELSDSR. This is not surprising for the Dieleman models, as they should be able to learn features similar or better than the MFCC features. However the logistic regression model also performs better, this is perhaps expected as sex classification don't require a lot of information, thus mean frequencies may suffice. Finally the logistic regression learns using all observations simultaneously, while GMM learns from just one sex at a time which could result in a worse performance.

In the speaker classification case (\cref{tab:results-speaker}) the GMM also performs worse than logistic regression, though in the ELSDSR case it is not significant. This is more surprising as speaker classification should be much harder and thus require a larger feature space, but this does not appear to be the case. Finally the Dieleman model performs significantly worse than logistic regression which is perhaps the most surprising result. At the very least, one would expect the Dieleman model to be able to learn something similar to the logistic regression. This could suggest that there may be a vanishing moment problem, however the network is able to overfit and the output distribution of the individual layers don't support this hypothesis. It is also possible that that there weren't enough observations per speaker for a CNN to be applicable.

Of all the different regularizations which were tried, the only one which slightly improved the Dieleman model was weight decay. In the original Dieleman paper \cite{dieleman} it was found that weight decay did not improve the performance. We suspect it may also be the case for our problem since our weight decay optimization contains 27 choices ($\lambda \in [10^{-10}, 5 \cdot 10^{-2}]$). Thus with a $95\%$ confidence interval we can expect $1.35$ of those tests to be a type 1 error (false rejection). We suspect that $\lambda = 5 \cdot 10^{-3}$ may be a false rejection.

\subsection{Generated datasets}
Rather surprisingly no instances were found where adding an invariance regularization improved performance. For instance, it was expected that the network would perform better with scale invariance on the xor-dataset because any point should be scalable and still be the same class (not accounting the noise). However, as seen from \cref{fig:reg_opt} and \cref{appendix:generated-contour-optimized} the regularizers do not appear to have any effect.

If one chooses extreme regularization parameters the contours do change as expected. In \cref{plt:generated-contour-extream} this is very apparent for the offset invariant regularizer that prefers contour lines going from bottom left to top right. The scale invariant also performs very poorly on the circles dataset, which is expected as it is the scale that creates the boundary.

An explanation for the poor performance may be the choice of the class outputs in the penalty term ($C_{i,k}$). In this paper, for each observation we chose the output-neuron matching the target class $t$ for the observation (i.e. $k=$t. One could argue that if the network is errorneously giving the target class a low probability then the derivate might also be low. Some experiments were made using the output-neuron with maximum activation ($\max_k C_{i,k} \forall i$) but it did not seem to make a difference.
