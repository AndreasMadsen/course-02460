%!TEX root = main.tex
\section{Discussion}
\subsection{TIMIT \& ELSDR}
In the sex classification case (\cref{tab:results-sex}) it is seen that the network models perform better than the GMM on MFCC for both TIMIT and ELSDSR. This is not surprising for the Dieleman models, as they should be able to learn features similar or better than the MFCC features. However, the logistic regression model also performs better. Perhaps this is expected as sex classification do not require a lot of information and thus mean frequencies may suffice. Finally, the logistic regression learns using all observations simultaneously while GMM learns from just one sex at a time which could be an attributing factor for the performance difference.

In the speaker classification case (\cref{tab:results-speaker}) the GMM also performs worse than logistic regression, though in the ELSDSR case it is not significant. This is more surprising as speaker classification should be a much harder problem and thus require a larger feature space. However, this does not appear to be the case. Finally, the Dieleman model performs significantly worse than logistic regression which is the most surprising result. At the very least, one would expect the Dieleman model to be able to learn something similar to the logistic regression. This could suggest that there may be a vanishing moment problem, however, the network is able to overfit and the output distribution of the individual layers don't support this hypothesis. It is also possible that that not enough observations per speaker were available for a CNN to be applicable.

Of all the different regularizations which were tried, the only one which slightly improved the Dieleman model was weight decay. In the original Dieleman paper \cite{dieleman} it was found that weight decay did not improve the performance. We suspect it may also be the case for our problem since our weight decay optimization contains 27 choices ($\lambda \in [10^{-10}, 5 \cdot 10^{-2}]$). Thus with a $95\%$ confidence interval we can expect $1.35$ of those tests to be a type 1 error (false rejection). We suspect that $\lambda = 5 \cdot 10^{-3}$ may be a false rejection.

\subsection{Generated datasets}
Rather surprisingly no instances were found where adding an invariance regularization improved performance. For instance, it was expected that the network would perform better with scale invariance on the xor-dataset because any point should be scalable and still be the same class (not accounting the noise). However, as seen from \cref{fig:reg_opt} and \cref{appendix:generated-contour-optimized} the regularizers do not appear to have any effect.

If one chooses extreme regularization parameters the contours do change as expected. In \cref{plt:generated-contour-extream} this is very apparent for the offset invariant regularizer that prefers contour lines going from bottom left to top right. The scale invariant regularizer also performs very poorly on the circles dataset which is expected seeing as it is the scale that creates the boundary.

An explanation for the poor performance may be the choice of the class outputs in the penalty term ($C_{i,k}$). In this paper, for each observation the output-neuron matching the target class $t$ was chosen (i.e. $k=$t). One could argue that if the network is errorneously giving the target class a low probability then the derivate might also be low. Some experiments were made using the output-neuron with maximum activation ($\max_k C_{i,k} \forall i$) but it did not seem to make a difference.
