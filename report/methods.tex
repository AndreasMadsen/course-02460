%!TEX root = main.tex
\section{Materials and Methods}

\subsection{Datasets: ELSDSR and TIMIT}

Two datasets were used for speaker classification, ELSDSR and TIMIT. The ELSDSR dataset includes 23 people and was specifically created for automatic speaker recognition. The TIMIT dataset is an acoustic-phonetic speech corpus and was thus not directly created for automatic speaker recognition. The TIMIT dataset thus have people with only one or few observations. To accommodate for this the dataset was filtered to only include people with sufficiently many long observations. After this filtering only 81 people was left.

\subsection{The Dieleman network}

\todo[inline]{Convolutional, Max Pool, ReLU, lasagne, theano. Overall network.}

\subsection{Scale Invariant Regualization}

The cross entropy loss function for classification is given as:
\begin{equation}
\mathcal{L}_{entropy} = - \frac{1}{N} \sum_{i=1}^N \sum_{k=1}^K t_{i,k} \ln(P(C_{i,k} | x_i, w))
\end{equation}

The approach used in \cite{scale-invariante}, is then to add a transformation invariant regularizer to this loss function. The transformation is given as $s(x, \alpha)$ where $x$ is the observation and $\alpha$ is the transformation parameter.
\begin{align}
\text{scale transformation:}&\quad s(x, \alpha) = \alpha x\\
\text{offset transformation:}&\quad s(x, \alpha) = x + \alpha
\end{align}

The regularizer then penalizes small changes in the output function (in this case $P(C_{i, k})$), by minimizing the derivative of the output function with respect to $\alpha$.
\begin{equation}
\mathcal{R}(s) = \frac{1}{N} \sum_{i=1}^N \left. \frac{\partial P(C_{i, k} | s(x_i, \alpha), w)}{\partial \alpha} \right|^2_{\alpha=0}
\end{equation}
By using the chain rule the derivative can be expressed as:
\begin{equation*}
\left. \frac{\partial P(C_{i, k} | s(x_i, \alpha), w)}{\partial \alpha} \right|_{\alpha=0} = \left.\nabla_x P(C_{i, k} | x_i, w) \frac{\partial s(x_i, \alpha)}{\partial \alpha} \right|_{\alpha=0}
\end{equation*}

This is extremely convenient from an implementation perspective, as $P(C_{i, k} | x_i, w)$ is already expressed as a theano graph. Note that this only works if $s(x, 0) = x$, which is not the case for the scale transformation. However the idea is simply that the derivative should be small for $\alpha$'s around the identity transformation, which for scaling is not $0$ but $1$. Thus it makes sense to substitute $\alpha = 0$ with $\alpha = 1$ for scaling, in which case the formulation do hold. The regularizers are then:
\begin{align}
\text{scale invariant: }& \mathcal{R} = \frac{1}{N} \sum_{i=1}^N \left(\nabla_x P(C_{i, k} | x_i, w)\bigcdot x_i\right)^2 \\
\text{offset invariant: }& \mathcal{R} = \frac{1}{N} \sum_{i=1}^N \left(\nabla_x P(C_{i, k} | x_i, w) + 1\right)^2
\end{align}

The full loss function then $\mathcal{L} = \mathcal{L}_{entropy} + \lambda \mathcal{R}$, where $\lambda$ is the regularization parameter.
