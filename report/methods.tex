%!TEX root = main.tex
\section{Materials and Methods}

\subsection{Datasets: ELSDSR and TIMIT}

Two datasets were used for speaker classification; ELSDSR and TIMIT. The ELSDSR dataset includes 23 people and was specifically created for automatic speaker recognition. The TIMIT dataset is an acoustic-phonetic speech corpus and was thus not directly created for automatic speaker recognition. As a result, the TIMIT dataset has people with few observations or very short soundclips. To accommodate for this the dataset was filtered to only include people with sufficiently many long observations. After this filtering only 81 people were left.

\subsection{The Dieleman network}

The Dieleman network, expecting a spectrogram $x \in \mathbb{R}^{m \times n}$ as input, consists of 6 layers: 2 $\times$(\emph{Convolutional} layer + \emph{Max Pooling} layer), followed by 2 consecutive fully connected \emph{Dense} layers.

The 1st Convolutional layer contains $32$ filters (kernels) of dimension $m \times 8$. Each filter only convolutes along the time axis with a stride of $1$ computing a single value for each stride offset, resulting in an output of size $n - \frac{8}{2} + 1 = n - 3$ for each filter i.e. the output of layer 1 is of dimension $32 \times (n-3)$. This is illustrated in \cref{fig:convolution-1}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{inkscape/convolution.pdf}
  \caption{Illustration of 1st convolution layer for a single filter.}
  \label{fig:convolution-1}
\end{figure}

The 2nd layer performs a 1-dimensional max pooling of size $1 \times 4$ on the output from layer 1, i.e. for each filter in layer 1, a window of size $1 \times 4$ is slid through the result with a stride offset of $4$ extracting the maximum value in the window.

This reduces the dimensionality of the output by $4$, thus giving a layer output of size $32 \times \frac{1}{4}(n-3)$.

For the 3rd and 4th network layer the same procedure is applied using filters of size $1 \times 8$, and again a max pooling of size $1 \times 4$.

Finally the output of layer 4 is feed into a regular fully-connected dense layer of $100$ nodes, and then a softmax function is used as the final layer.

On the \emph{Convolutional} and \emph{Dense} layers the rectifier function (\cref{eq:rectifier}) is used as activation function
\begin{equation}
  f(x) = \text{max(}0, x\text{)},
  \label{eq:rectifier}
\end{equation}
i.e. the layers uses rectified linear units (ReLU).

The network is implemented in \texttt{python} using the library \texttt{lasagne}, which is implemented on top of \texttt{Theano} which allows for the use of highly optimized mathematical expression implementations, and also allows for transparent use of GPUs \cite{theano}.

\subsection{Scale Invariant Regualization}

The cross entropy loss function for classification is given as:
\begin{equation}
\mathcal{L}_{entropy} = - \frac{1}{N} \sum_{i=1}^N \sum_{k=1}^K t_{i,k} \ln(P(C_{i,k} | x_i, w))
\end{equation}

The approach used in \cite{scale-invariante}, is to add a transformation invariant regularizer to this loss function. The transformation is given as $s(x, \alpha)$ where $x$ is the observation and $\alpha$ is the transformation parameter. The only restriction on $s$ is that $s(x, \alpha) |_{\alpha=0} =x$. As such two examples of $s$ are:
\begin{align}
\text{scale transformation:}&\quad s(x, \alpha) = (1+\alpha) x\\
\text{offset transformation:}&\quad s(x, \alpha) = x + \alpha
\end{align}

The regularizer then penalizes changes in the output function (in this case $P(C_{i, k})$), by minimizing the derivative of the output function with respect to $\alpha$.
\begin{equation}
\mathcal{R}(s) = \frac{1}{N} \sum_{i=1}^N \left. \frac{\partial P(C_{i, k} | s(x_i, \alpha), w)}{\partial \alpha} \right|^2_{\alpha=0}
\end{equation}
By using the chain rule the derivative can be expressed as:
\begin{equation*}
\left. \frac{\partial P(C_{i, k} | s(x_i, \alpha), w)}{\partial \alpha} \right|_{\alpha=0} = \left.\nabla_x P(C_{i, k} |  x_i, w) \frac{\partial s(x_i, \alpha)}{\partial \alpha} \right|_{\alpha=0}
\end{equation*}

This is extremely convenient from an implementation perspective, as $P(C_{i, k} | x_i, w)$ is already expressed as a Theano graph. More concretely for the scale and offset invariance the regularizers become:
\begin{align}
\text{scale invariant: }& \mathcal{R} = \frac{1}{N} \sum_{i=1}^N \left(\nabla_x P(C_{i, k} | x_i, w) \bigcdot x_i\right)^2 \\
\text{offset invariant: }& \mathcal{R} = \frac{1}{N} \sum_{i=1}^N \left(\nabla_x P(C_{i, k} | x_i, w) \bigcdot \mathbf{1}\right)^2
\end{align}

The full loss function then $\mathcal{L} = \mathcal{L}_{entropy} + \lambda \mathcal{R}$, where $\lambda$ is a regularization parameter.
